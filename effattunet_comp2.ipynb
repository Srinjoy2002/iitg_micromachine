{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-25T04:09:18.460203Z","iopub.execute_input":"2025-06-25T04:09:18.460446Z","iopub.status.idle":"2025-06-25T04:09:20.005244Z","shell.execute_reply.started":"2025-06-25T04:09:18.460423Z","shell.execute_reply":"2025-06-25T04:09:20.004448Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"!pip install roboflow\n\n\n# Install dependencies\n!pip install roboflow scikit-learn pandas\n\nimport os, cv2, math, numpy as np, pandas as pd\nfrom glob import glob\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.layers import (\n    Conv2D, BatchNormalization, Activation,\n    UpSampling2D, Concatenate, Dropout,\n    Input, Lambda, Multiply\n)\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import (\n    ModelCheckpoint, ReduceLROnPlateau, LearningRateScheduler\n)\nfrom tensorflow.keras.metrics import MeanIoU\nfrom sklearn.model_selection import KFold\nfrom roboflow import Roboflow\n\n# ─────────────────────────────────────────\n# 0) Download Roboflow YOLO dataset\n# ─────────────────────────────────────────\nfrom roboflow import Roboflow\nrf = Roboflow(api_key=\"qz3gXKPbOfhvVaI8oDt4\")\nproject = rf.workspace(\"rkm-nnbdx\").project(\"3d-reconstruction-ga5qp\")\nversion = project.version(1)\ndataset = version.download(\"yolov7\") # images + labels/*.txt\n\nBASE       = dataset.location            # e.g. \"/kaggle/working/Microfocus-8\"\nIMG_SIZE   = (384, 384)\nBATCH_SIZE = 4\nAUTOTUNE   = tf.data.AUTOTUNE\nEPOCHS     = 100\nSEED       = 42\nLR         = 1e-4\nFOLDS      = 2\nSMOOTH_W   = 7\n\ntf.random.set_seed(SEED)\nnp.random.seed(SEED)\n\n# ─────────────────────────────────────────\n# 1) Generate PNG masks from YOLO polygons\n# ─────────────────────────────────────────\ndef yolo_poly_to_mask(img_path, label_path, out_path, size=IMG_SIZE):\n    w,h = size\n    mask = np.zeros((h, w), dtype=np.uint8)\n    if os.path.exists(label_path):\n        for line in open(label_path):\n            pts = np.array(list(map(float, line.split()[1:])), dtype=np.float32).reshape(-1,2)\n            pts[:,0] *= w; pts[:,1] *= h\n            cv2.fillPoly(mask, [pts.astype(np.int32)], 255)\n    cv2.imwrite(out_path, mask)\n\nfor split in (\"train\",\"valid\",\"test\"):\n    img_dir = os.path.join(BASE, split, \"images\")\n    lbl_dir = os.path.join(BASE, split, \"labels\")\n    msk_dir = os.path.join(BASE, split, \"masks\")\n    os.makedirs(msk_dir, exist_ok=True)\n    for img_p in tqdm(glob(f\"{img_dir}/*.jpg\"), desc=f\"Make masks for {split}\"):\n        fn    = os.path.splitext(os.path.basename(img_p))[0]\n        lbl_p = os.path.join(lbl_dir, fn + \".txt\")\n        out_p = os.path.join(msk_dir, fn + \".png\")\n        yolo_poly_to_mask(img_p, lbl_p, out_p)\n\n# ─────────────────────────────────────────\n# 2) Data pipeline utilities\n# ─────────────────────────────────────────\ndef load_pair(img_path, mask_path):\n    img = tf.io.read_file(img_path)\n    img = tf.image.decode_jpeg(img, 3)\n    img = tf.image.resize(img, IMG_SIZE) / 255.0\n\n    m = tf.io.read_file(mask_path)\n    m = tf.image.decode_png(m, 1)\n    m = tf.image.resize(m, IMG_SIZE, method=\"nearest\")\n    m = tf.cast(m > 127, tf.float32)\n    return img, m\n\ndef augment(img, m):\n    cat = tf.concat([img, m], axis=-1)\n    if tf.random.uniform([]) > 0.5: cat = tf.image.flip_left_right(cat)\n    if tf.random.uniform([]) > 0.5: cat = tf.image.flip_up_down(cat)\n    k = tf.random.uniform([], 0, 4, tf.int32)\n    cat = tf.image.rot90(cat, k)\n    im, ma = cat[...,:3], cat[...,3:]\n    im = tf.image.random_brightness(im, 0.1)\n    im = tf.image.random_contrast(im, 0.9, 1.1)\n    ma = tf.cast(ma>0.5, tf.float32)\n    return im, ma\n\n# **Single** make_ds signature:\ndef make_ds(img_list, mask_list, train=False):\n    ds = tf.data.Dataset.from_tensor_slices((img_list, mask_list))\n    ds = ds.map(load_pair, num_parallel_calls=AUTOTUNE)\n    if train:\n        ds = ds.map(augment, num_parallel_calls=AUTOTUNE)\n        ds = ds.shuffle(200, seed=SEED).batch(BATCH_SIZE).repeat()\n    else:\n        ds = ds.batch(BATCH_SIZE)\n    return ds.prefetch(AUTOTUNE)\n\n# Build train/val/test sets (not used in backbone CV, but here for sanity)\ntrain_ds = make_ds(glob(f\"{BASE}/train/images/*.jpg\"),\n                   [p.replace(\"/images/\",\"/masks/\").replace(\".jpg\",\".png\")\n                    for p in glob(f\"{BASE}/train/images/*.jpg\")],\n                   train=True)\nval_ds   = make_ds(glob(f\"{BASE}/valid/images/*.jpg\"),\n                   [p.replace(\"/images/\",\"/masks/\").replace(\".jpg\",\".png\")\n                    for p in glob(f\"{BASE}/valid/images/*.jpg\")],\n                   train=False)\ntest_ds  = make_ds(glob(f\"{BASE}/test/images/*.jpg\"),\n                   [p.replace(\"/images/\",\"/masks/\").replace(\".jpg\",\".png\")\n                    for p in glob(f\"{BASE}/test/images/*.jpg\")],\n                   train=False)\n\n# ─────────────────────────────────────────\n# 3) Loss & metrics\n# ─────────────────────────────────────────\ndef dice_loss(y_true,y_pred, smooth=1e-6):\n    yt, yp = K.flatten(y_true), K.flatten(y_pred)\n    inter = K.sum(yt*yp)\n    return 1 - (2*inter+smooth)/(K.sum(yt)+K.sum(yp)+smooth)\n\ndef tversky(y_true,y_pred, alpha=0.3,beta=0.7,smooth=1e-6):\n    yt, yp = K.flatten(y_true), K.flatten(y_pred)\n    tp = K.sum(yt*yp); fp = K.sum((1-yt)*yp); fn = K.sum(yt*(1-yp))\n    return (tp+smooth)/(tp+alpha*fp+beta*fn+smooth)\n\ndef focal_tversky(y_true,y_pred):\n    t = tversky(y_true,y_pred)\n    return K.pow((1-t),0.75)\n\ndef combined_loss(y_true,y_pred):\n    return 0.4*dice_loss(y_true,y_pred) + 0.6*focal_tversky(y_true,y_pred)\n\nmetrics = ['accuracy', MeanIoU(num_classes=2,name='mean_iou')]\n\n# ─────────────────────────────────────────\n# 4) Attention‑U‑Net builder\n# ─────────────────────────────────────────\ndef attention_gate(skip, g, inter):\n    θ = Conv2D(inter,1,padding='same')(skip)\n    φ = Conv2D(inter,1,padding='same')(g)\n    up= Lambda(lambda x: tf.image.resize(x[0], tf.shape(x[1])[1:3]))([φ, skip])\n    f = Activation('relu')(θ+up)\n    ψ = Conv2D(1,1,padding='same',activation='sigmoid')(f)\n    return Multiply()([skip, ψ])\n\ndef build_effattunet(backbone):\n    inp = Input((*IMG_SIZE,3))\n    enc = backbone(include_top=False, weights='imagenet', input_tensor=inp)\n    skips = [enc.get_layer(n).output for n in (\n      \"block2a_expand_activation\",\n      \"block3a_expand_activation\",\n      \"block4a_expand_activation\",\n      \"block6a_expand_activation\"\n    )]\n    x = enc.output\n    filters=[256,128,64,32]\n    for i, skip in enumerate(reversed(skips)):\n        x = UpSampling2D()(x)\n        ag = attention_gate(skip, x, inter=filters[i]//2)\n        x  = Concatenate()([x, ag])\n        x  = Conv2D(filters[i],3,padding='same',activation='relu')(x)\n        x  = BatchNormalization()(x)\n        x  = Conv2D(filters[i],3,padding='same',activation='relu')(x)\n        x  = BatchNormalization()(x)\n        x  = Dropout(0.3)(x)\n    # final upsampling to full resolution\n    x = UpSampling2D()(x)\n    x = Conv2D(32,3,padding='same',activation='relu')(x)\n    x = BatchNormalization()(x)\n    x = Conv2D(32,3,padding='same',activation='relu')(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.3)(x)\n    out = Conv2D(1,1,activation='sigmoid')(x)\n    return Model(inp,out)\n\n# ─────────────────────────────────────────\n# 5) Cross‑validation per backbone\n# ─────────────────────────────────────────\nbackbones = {\n  # 'EffB1': tf.keras.applications.EfficientNetB1,\n  # 'EffB2': tf.keras.applications.EfficientNetB2,\n  'EffB5': tf.keras.applications.EfficientNetB5\n}\n\nall_imgs  = sorted(glob(f\"{BASE}/train/images/*.jpg\"))\nall_masks = [p.replace(\"/images/\",\"/masks/\").replace(\".jpg\",\".png\")\n             for p in all_imgs]\nkf = KFold(n_splits=FOLDS, shuffle=True, random_state=SEED)\n\nresults = {name: [] for name in backbones}\nfor name, eff in backbones.items():\n    print(f\"\\n=== {name} ===\")\n    for fold, (trIdx, valIdx) in enumerate(kf.split(all_imgs)):\n        print(f\" Fold {fold+1}\")\n        trI = [all_imgs[i] for i in trIdx];  trM = [all_masks[i] for i in trIdx]\n        vI  = [all_imgs[i] for i in valIdx]; vM  = [all_masks[i] for i in valIdx]\n\n        dsTr  = make_ds(trI, trM, train=True)\n        dsVal = make_ds(vI, vM, train=False)\n\n        model = build_effattunet(eff)\n        model.compile(optimizer=Adam(LR),\n                      loss=combined_loss,\n                      metrics=metrics)\n\n        def cos_lr(e):\n            t = e % 30\n            return LR * 0.5 * (1 + math.cos(math.pi * t/30))\n        cbs = [\n          LearningRateScheduler(cos_lr, verbose=0),\n          ModelCheckpoint( f\"{name}_f{fold}.h5\",\n                           monitor=\"val_mean_iou\", mode=\"max\",\n                           save_best_only=True),\n          ReduceLROnPlateau( monitor=\"val_mean_iou\",\n                            factor=0.5, patience=30,\n                            mode=\"max\", verbose=0)\n        ]\n\n        steps = len(trI)//BATCH_SIZE\n        vstps = len(vI)//BATCH_SIZE\n        h = model.fit(dsTr,\n                      epochs=EPOCHS,\n                      steps_per_epoch=steps,\n                      validation_data=dsVal,\n                      validation_steps=vstps,\n                      callbacks=cbs,\n                      verbose=1)\n        model.load_weights(f\"{name}_f{fold}.h5\")\n        results[name].append(max(h.history['val_mean_iou']))\n\n    print(\" →\", name, \"mean val mIoU:\", np.mean(results[name]))\n\n# ─────────────────────────────────────────\n# 6) Plot backbone comparison\n# ─────────────────────────────────────────\nplt.figure(figsize=(6,4))\nnames  = list(results.keys())\nscores = [np.mean(results[n]) for n in names]\nplt.bar(names, scores, color=['#a6cee3','#1f78b4','#b2df8a'])\nplt.ylabel(\"Mean Val mIoU\")\nplt.ylim(0,1)\nfor i, v in enumerate(scores):\n    plt.text(i, v+0.01, f\"{v:.3f}\", ha='center')\nplt.title(\"Backbone comparison: EffB1 vs EffB2 vs EffB3\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T04:09:20.006592Z","iopub.execute_input":"2025-06-25T04:09:20.007172Z","execution_failed":"2025-06-25T06:05:42.773Z"}},"outputs":[{"name":"stdout","text":"Collecting roboflow\n  Downloading roboflow-1.1.66-py3-none-any.whl.metadata (9.7 kB)\nRequirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from roboflow) (2025.4.26)\nCollecting idna==3.7 (from roboflow)\n  Downloading idna-3.7-py3-none-any.whl.metadata (9.9 kB)\nRequirement already satisfied: cycler in /usr/local/lib/python3.11/dist-packages (from roboflow) (0.12.1)\nRequirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.4.8)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from roboflow) (3.7.2)\nRequirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.26.4)\nCollecting opencv-python-headless==4.10.0.84 (from roboflow)\n  Downloading opencv_python_headless-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\nRequirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from roboflow) (11.1.0)\nCollecting pillow-heif>=0.18.0 (from roboflow)\n  Downloading pillow_heif-0.22.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\nRequirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.9.0.post0)\nCollecting python-dotenv (from roboflow)\n  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.32.3)\nRequirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.17.0)\nRequirement already satisfied: urllib3>=1.26.6 in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.4.0)\nRequirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from roboflow) (4.67.1)\nRequirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from roboflow) (6.0.2)\nRequirement already satisfied: requests-toolbelt in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.0.0)\nCollecting filetype (from roboflow)\n  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.18.5->roboflow) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.18.5->roboflow) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.18.5->roboflow) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.18.5->roboflow) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.18.5->roboflow) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.18.5->roboflow) (2.4.1)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (1.3.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (4.57.0)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (25.0)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->roboflow) (3.4.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.18.5->roboflow) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.18.5->roboflow) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.18.5->roboflow) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.18.5->roboflow) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.18.5->roboflow) (2024.2.0)\nDownloading roboflow-1.1.66-py3-none-any.whl (86 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.7/86.7 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading idna-3.7-py3-none-any.whl (66 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.8/66.8 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading opencv_python_headless-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.9/49.9 MB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading pillow_heif-0.22.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m83.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\nDownloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\nInstalling collected packages: filetype, python-dotenv, pillow-heif, idna, opencv-python-headless, roboflow\n  Attempting uninstall: idna\n    Found existing installation: idna 3.10\n    Uninstalling idna-3.10:\n      Successfully uninstalled idna-3.10\n  Attempting uninstall: opencv-python-headless\n    Found existing installation: opencv-python-headless 4.11.0.86\n    Uninstalling opencv-python-headless-4.11.0.86:\n      Successfully uninstalled opencv-python-headless-4.11.0.86\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.3.2 which is incompatible.\ngoogle-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.40.1 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\ngoogle-spark-connect 0.5.2 requires google-api-core>=2.19.1, but you have google-api-core 1.34.1 which is incompatible.\ngoogle-cloud-bigtable 2.30.0 requires google-api-core[grpc]<3.0.0,>=2.16.0, but you have google-api-core 1.34.1 which is incompatible.\ngoogle-cloud-storage 2.19.0 requires google-api-core<3.0.0dev,>=2.15.0, but you have google-api-core 1.34.1 which is incompatible.\npandas-gbq 0.28.0 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed filetype-1.2.0 idna-3.7 opencv-python-headless-4.10.0.84 pillow-heif-0.22.0 python-dotenv-1.1.1 roboflow-1.1.66\nRequirement already satisfied: roboflow in /usr/local/lib/python3.11/dist-packages (1.1.66)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.2.2)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.3)\nRequirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from roboflow) (2025.4.26)\nRequirement already satisfied: idna==3.7 in /usr/local/lib/python3.11/dist-packages (from roboflow) (3.7)\nRequirement already satisfied: cycler in /usr/local/lib/python3.11/dist-packages (from roboflow) (0.12.1)\nRequirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.4.8)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from roboflow) (3.7.2)\nRequirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.26.4)\nRequirement already satisfied: opencv-python-headless==4.10.0.84 in /usr/local/lib/python3.11/dist-packages (from roboflow) (4.10.0.84)\nRequirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from roboflow) (11.1.0)\nRequirement already satisfied: pillow-heif>=0.18.0 in /usr/local/lib/python3.11/dist-packages (from roboflow) (0.22.0)\nRequirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.9.0.post0)\nRequirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.1.1)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.32.3)\nRequirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.17.0)\nRequirement already satisfied: urllib3>=1.26.6 in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.4.0)\nRequirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from roboflow) (4.67.1)\nRequirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from roboflow) (6.0.2)\nRequirement already satisfied: requests-toolbelt in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.0.0)\nRequirement already satisfied: filetype in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.2.0)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.2)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.18.5->roboflow) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.18.5->roboflow) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.18.5->roboflow) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.18.5->roboflow) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.18.5->roboflow) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.18.5->roboflow) (2.4.1)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (1.3.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (4.57.0)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (25.0)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->roboflow) (3.4.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.18.5->roboflow) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.18.5->roboflow) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.18.5->roboflow) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.18.5->roboflow) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.18.5->roboflow) (2024.2.0)\n","output_type":"stream"},{"name":"stderr","text":"2025-06-25 04:09:33.996421: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1750824574.232468      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1750824574.293804      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"loading Roboflow workspace...\nloading Roboflow project...\n","output_type":"stream"},{"name":"stderr","text":"Make masks for train: 100%|██████████| 3225/3225 [00:02<00:00, 1442.01it/s]\nMake masks for valid: 100%|██████████| 538/538 [00:00<00:00, 1454.80it/s]\nMake masks for test: 100%|██████████| 179/179 [00:00<00:00, 1486.84it/s]\nI0000 00:00:1750824591.739489      35 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\nI0000 00:00:1750824591.740207      35 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13942 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n","output_type":"stream"},{"name":"stdout","text":"\n=== EffB3 ===\n Fold 1\nDownloading data from https://storage.googleapis.com/keras-applications/efficientnetb3_notop.h5\n\u001b[1m43941136/43941136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\nEpoch 1/100\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1750824698.303578     110 service.cc:148] XLA service 0x7ddd440027e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1750824698.304731     110 service.cc:156]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\nI0000 00:00:1750824698.304752     110 service.cc:156]   StreamExecutor device (1): Tesla T4, Compute Capability 7.5\nI0000 00:00:1750824707.774968     110 cuda_dnn.cc:529] Loaded cuDNN version 90300\nE0000 00:00:1750824715.831487     110 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\nE0000 00:00:1750824715.976937     110 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\nE0000 00:00:1750824727.951438     110 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\nE0000 00:00:1750824728.087681     110 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\nI0000 00:00:1750824775.811296     110 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m403/403\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m314s\u001b[0m 337ms/step - accuracy: 0.7870 - loss: 0.3542 - mean_iou: 0.3533 - val_accuracy: 0.7239 - val_loss: 0.7076 - val_mean_iou: 0.3332 - learning_rate: 1.0000e-04\nEpoch 2/100\n\u001b[1m403/403\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 294ms/step - accuracy: 0.8560 - loss: 0.2634 - mean_iou: 0.3574 - val_accuracy: 0.8543 - val_loss: 0.4127 - val_mean_iou: 0.3332 - learning_rate: 9.9726e-05\nEpoch 3/100\n\u001b[1m403/403\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 293ms/step - accuracy: 0.8607 - loss: 0.2601 - mean_iou: 0.3653 - val_accuracy: 0.4573 - val_loss: 0.5226 - val_mean_iou: 0.3332 - learning_rate: 9.8907e-05\nEpoch 4/100\n\u001b[1m403/403\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 292ms/step - accuracy: 0.8732 - loss: 0.2366 - mean_iou: 0.3868 - val_accuracy: 0.8776 - val_loss: 0.3772 - val_mean_iou: 0.3332 - learning_rate: 9.7553e-05\nEpoch 5/100\n\u001b[1m403/403\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 293ms/step - accuracy: 0.8833 - loss: 0.2224 - mean_iou: 0.3984 - val_accuracy: 0.3628 - val_loss: 0.5731 - val_mean_iou: 0.3332 - learning_rate: 9.5677e-05\nEpoch 6/100\n\u001b[1m403/403\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 292ms/step - accuracy: 0.8785 - loss: 0.2290 - mean_iou: 0.4153 - val_accuracy: 0.8725 - val_loss: 0.3735 - val_mean_iou: 0.3332 - learning_rate: 9.3301e-05\nEpoch 7/100\n\u001b[1m403/403\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 293ms/step - accuracy: 0.8996 - loss: 0.1987 - mean_iou: 0.4422 - val_accuracy: 0.5809 - val_loss: 0.6071 - val_mean_iou: 0.3332 - learning_rate: 9.0451e-05\nEpoch 8/100\n\u001b[1m403/403\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 293ms/step - accuracy: 0.9032 - loss: 0.2016 - mean_iou: 0.4966 - val_accuracy: 0.8877 - val_loss: 0.3897 - val_mean_iou: 0.3332 - learning_rate: 8.7157e-05\nEpoch 9/100\n\u001b[1m403/403\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 292ms/step - accuracy: 0.9104 - loss: 0.1855 - mean_iou: 0.5240 - val_accuracy: 0.5555 - val_loss: 0.8671 - val_mean_iou: 0.3332 - learning_rate: 8.3457e-05\nEpoch 10/100\n\u001b[1m403/403\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 293ms/step - accuracy: 0.9201 - loss: 0.1708 - mean_iou: 0.5445 - val_accuracy: 0.8372 - val_loss: 0.5432 - val_mean_iou: 0.3332 - learning_rate: 7.9389e-05\nEpoch 11/100\n\u001b[1m403/403\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 292ms/step - accuracy: 0.9157 - loss: 0.1835 - mean_iou: 0.5548 - val_accuracy: 0.6645 - val_loss: 0.6820 - val_mean_iou: 0.3332 - learning_rate: 7.5000e-05\nEpoch 12/100\n\u001b[1m403/403\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 292ms/step - accuracy: 0.9243 - loss: 0.1662 - mean_iou: 0.6093 - val_accuracy: 0.8727 - val_loss: 0.4495 - val_mean_iou: 0.3332 - learning_rate: 7.0337e-05\nEpoch 13/100\n\u001b[1m403/403\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 292ms/step - accuracy: 0.9261 - loss: 0.1653 - mean_iou: 0.5999 - val_accuracy: 0.8756 - val_loss: 0.4079 - val_mean_iou: 0.3332 - learning_rate: 6.5451e-05\nEpoch 14/100\n\u001b[1m403/403\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 291ms/step - accuracy: 0.9239 - loss: 0.1609 - mean_iou: 0.6254 - val_accuracy: 0.8943 - val_loss: 0.3618 - val_mean_iou: 0.3332 - learning_rate: 6.0396e-05\nEpoch 15/100\n\u001b[1m403/403\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 296ms/step - accuracy: 0.9267 - loss: 0.1577 - mean_iou: 0.6106 - val_accuracy: 0.6089 - val_loss: 0.8330 - val_mean_iou: 0.3338 - learning_rate: 5.5226e-05\nEpoch 16/100\n\u001b[1m403/403\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 292ms/step - accuracy: 0.9332 - loss: 0.1520 - mean_iou: 0.6725 - val_accuracy: 0.9027 - val_loss: 0.3410 - val_mean_iou: 0.3332 - learning_rate: 5.0000e-05\nEpoch 17/100\n\u001b[1m403/403\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 292ms/step - accuracy: 0.9328 - loss: 0.1509 - mean_iou: 0.6712 - val_accuracy: 0.8845 - val_loss: 0.4183 - val_mean_iou: 0.3332 - learning_rate: 4.4774e-05\nEpoch 18/100\n\u001b[1m403/403\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 292ms/step - accuracy: 0.9293 - loss: 0.1606 - mean_iou: 0.6664 - val_accuracy: 0.8657 - val_loss: 0.4777 - val_mean_iou: 0.3332 - learning_rate: 3.9604e-05\nEpoch 19/100\n\u001b[1m403/403\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 292ms/step - accuracy: 0.9358 - loss: 0.1443 - mean_iou: 0.6906 - val_accuracy: 0.8766 - val_loss: 0.4329 - val_mean_iou: 0.3332 - learning_rate: 3.4549e-05\nEpoch 20/100\n\u001b[1m403/403\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 292ms/step - accuracy: 0.9376 - loss: 0.1448 - mean_iou: 0.7038 - val_accuracy: 0.8716 - val_loss: 0.3757 - val_mean_iou: 0.3332 - learning_rate: 2.9663e-05\nEpoch 21/100\n\u001b[1m403/403\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 292ms/step - accuracy: 0.9408 - loss: 0.1345 - mean_iou: 0.7048 - val_accuracy: 0.9088 - val_loss: 0.3380 - val_mean_iou: 0.3332 - learning_rate: 2.5000e-05\nEpoch 22/100\n\u001b[1m403/403\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 292ms/step - accuracy: 0.9384 - loss: 0.1384 - mean_iou: 0.7166 - val_accuracy: 0.9082 - val_loss: 0.3570 - val_mean_iou: 0.3332 - learning_rate: 2.0611e-05\nEpoch 23/100\n\u001b[1m403/403\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 292ms/step - accuracy: 0.9406 - loss: 0.1399 - mean_iou: 0.7286 - val_accuracy: 0.9259 - val_loss: 0.3231 - val_mean_iou: 0.3332 - learning_rate: 1.6543e-05\nEpoch 24/100\n\u001b[1m403/403\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 292ms/step - accuracy: 0.9438 - loss: 0.1268 - mean_iou: 0.7235 - val_accuracy: 0.9265 - val_loss: 0.3234 - val_mean_iou: 0.3332 - learning_rate: 1.2843e-05\nEpoch 25/100\n\u001b[1m403/403\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 292ms/step - accuracy: 0.9402 - loss: 0.1382 - mean_iou: 0.7261 - val_accuracy: 0.9249 - val_loss: 0.3309 - val_mean_iou: 0.3332 - learning_rate: 9.5492e-06\nEpoch 26/100\n\u001b[1m403/403\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 292ms/step - accuracy: 0.9435 - loss: 0.1323 - mean_iou: 0.7169 - val_accuracy: 0.9279 - val_loss: 0.3174 - val_mean_iou: 0.3332 - learning_rate: 6.6987e-06\nEpoch 27/100\n\u001b[1m403/403\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 292ms/step - accuracy: 0.9415 - loss: 0.1342 - mean_iou: 0.7351 - val_accuracy: 0.9313 - val_loss: 0.3082 - val_mean_iou: 0.3332 - learning_rate: 4.3227e-06\nEpoch 28/100\n\u001b[1m403/403\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 292ms/step - accuracy: 0.9436 - loss: 0.1325 - mean_iou: 0.7223 - val_accuracy: 0.9302 - val_loss: 0.3059 - val_mean_iou: 0.3332 - learning_rate: 2.4472e-06\nEpoch 29/100\n\u001b[1m403/403\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 292ms/step - accuracy: 0.9453 - loss: 0.1307 - mean_iou: 0.7527 - val_accuracy: 0.9309 - val_loss: 0.3079 - val_mean_iou: 0.3332 - learning_rate: 1.0926e-06\nEpoch 30/100\n\u001b[1m403/403\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 292ms/step - accuracy: 0.9447 - loss: 0.1281 - mean_iou: 0.7463 - val_accuracy: 0.9310 - val_loss: 0.3079 - val_mean_iou: 0.3332 - learning_rate: 2.7391e-07\nEpoch 31/100\n\u001b[1m403/403\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 292ms/step - accuracy: 0.9327 - loss: 0.1515 - mean_iou: 0.7158 - val_accuracy: 0.7724 - val_loss: 0.7195 - val_mean_iou: 0.3332 - learning_rate: 1.0000e-04\nEpoch 32/100\n\u001b[1m403/403\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 292ms/step - accuracy: 0.9324 - loss: 0.1546 - mean_iou: 0.7096 - val_accuracy: 0.6113 - val_loss: 0.5633 - val_mean_iou: 0.3332 - learning_rate: 9.9726e-05\nEpoch 33/100\n\u001b[1m403/403\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 292ms/step - accuracy: 0.9327 - loss: 0.1550 - mean_iou: 0.7278 - val_accuracy: 0.7339 - val_loss: 0.6214 - val_mean_iou: 0.3332 - learning_rate: 9.8907e-05\nEpoch 34/100\n\u001b[1m403/403\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 292ms/step - accuracy: 0.9326 - loss: 0.1577 - mean_iou: 0.7331 - val_accuracy: 0.6728 - val_loss: 0.9766 - val_mean_iou: 0.3334 - learning_rate: 9.7553e-05\nEpoch 35/100\n\u001b[1m403/403\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 292ms/step - accuracy: 0.9345 - loss: 0.1484 - mean_iou: 0.7474 - val_accuracy: 0.8131 - val_loss: 0.6425 - val_mean_iou: 0.3332 - learning_rate: 9.5677e-05\nEpoch 36/100\n\u001b[1m403/403\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 356ms/step - accuracy: 0.9347 - loss: 0.1510 - mean_iou: 0.7576 - val_accuracy: 0.8184 - val_loss: 0.5644 - val_mean_iou: 0.3343 - learning_rate: 9.3301e-05\nEpoch 37/100\n\u001b[1m403/403\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 292ms/step - accuracy: 0.9390 - loss: 0.1362 - mean_iou: 0.7702 - val_accuracy: 0.8477 - val_loss: 0.4629 - val_mean_iou: 0.3335 - learning_rate: 9.0451e-05\nEpoch 38/100\n\u001b[1m403/403\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 295ms/step - accuracy: 0.9346 - loss: 0.1475 - mean_iou: 0.7699 - val_accuracy: 0.6557 - val_loss: 0.9728 - val_mean_iou: 0.3355 - learning_rate: 8.7157e-05\nEpoch 39/100\n\u001b[1m403/403\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 292ms/step - accuracy: 0.9369 - loss: 0.1412 - mean_iou: 0.7879 - val_accuracy: 0.8763 - val_loss: 0.4391 - val_mean_iou: 0.3337 - learning_rate: 8.3457e-05\nEpoch 40/100\n\u001b[1m403/403\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 292ms/step - accuracy: 0.9401 - loss: 0.1400 - mean_iou: 0.7872 - val_accuracy: 0.8733 - val_loss: 0.4677 - val_mean_iou: 0.3345 - learning_rate: 7.9389e-05\nEpoch 41/100\n\u001b[1m403/403\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 292ms/step - accuracy: 0.9428 - loss: 0.1309 - mean_iou: 0.7948 - val_accuracy: 0.6677 - val_loss: 0.9497 - val_mean_iou: 0.3345 - learning_rate: 7.5000e-05\nEpoch 42/100\n\u001b[1m403/403\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 295ms/step - accuracy: 0.9408 - loss: 0.1353 - mean_iou: 0.8097 - val_accuracy: 0.8020 - val_loss: 0.6436 - val_mean_iou: 0.3605 - learning_rate: 7.0337e-05\nEpoch 43/100\n\u001b[1m403/403\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 296ms/step - accuracy: 0.9455 - loss: 0.1251 - mean_iou: 0.8166 - val_accuracy: 0.8519 - val_loss: 0.4983 - val_mean_iou: 0.3643 - learning_rate: 6.5451e-05\nEpoch 44/100\n\u001b[1m403/403\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 292ms/step - accuracy: 0.9465 - loss: 0.1240 - mean_iou: 0.8259 - val_accuracy: 0.8869 - val_loss: 0.4207 - val_mean_iou: 0.3334 - learning_rate: 6.0396e-05\nEpoch 45/100\n\u001b[1m403/403\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 292ms/step - accuracy: 0.9436 - loss: 0.1326 - mean_iou: 0.7991 - val_accuracy: 0.8988 - val_loss: 0.3821 - val_mean_iou: 0.3396 - learning_rate: 5.5226e-05\nEpoch 46/100\n\u001b[1m403/403\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 292ms/step - accuracy: 0.9436 - loss: 0.1260 - mean_iou: 0.8246 - val_accuracy: 0.8425 - val_loss: 0.5311 - val_mean_iou: 0.3495 - learning_rate: 5.0000e-05\nEpoch 47/100\n\u001b[1m403/403\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 296ms/step - accuracy: 0.9470 - loss: 0.1266 - mean_iou: 0.8292 - val_accuracy: 0.8671 - val_loss: 0.4620 - val_mean_iou: 0.4109 - learning_rate: 4.4774e-05\nEpoch 48/100\n\u001b[1m403/403\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 292ms/step - accuracy: 0.9442 - loss: 0.1297 - mean_iou: 0.8173 - val_accuracy: 0.9218 - val_loss: 0.3273 - val_mean_iou: 0.3905 - learning_rate: 3.9604e-05\nEpoch 49/100\n\u001b[1m403/403\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 291ms/step - accuracy: 0.9483 - loss: 0.1179 - mean_iou: 0.8376 - val_accuracy: 0.7330 - val_loss: 0.8006 - val_mean_iou: 0.4009 - learning_rate: 3.4549e-05\nEpoch 50/100\n\u001b[1m403/403\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 296ms/step - accuracy: 0.9485 - loss: 0.1175 - mean_iou: 0.8310 - val_accuracy: 0.8604 - val_loss: 0.5072 - val_mean_iou: 0.4742 - learning_rate: 2.9663e-05\nEpoch 51/100\n\u001b[1m403/403\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 293ms/step - accuracy: 0.9491 - loss: 0.1150 - mean_iou: 0.8390 - val_accuracy: 0.9269 - val_loss: 0.3194 - val_mean_iou: 0.4220 - learning_rate: 2.5000e-05\nEpoch 52/100\n\u001b[1m403/403\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 296ms/step - accuracy: 0.9514 - loss: 0.1102 - mean_iou: 0.8281 - val_accuracy: 0.9196 - val_loss: 0.3364 - val_mean_iou: 0.7135 - learning_rate: 2.0611e-05\nEpoch 53/100\n\u001b[1m403/403\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 296ms/step - accuracy: 0.9484 - loss: 0.1173 - mean_iou: 0.8275 - val_accuracy: 0.9231 - val_loss: 0.3298 - val_mean_iou: 0.8370 - learning_rate: 1.6543e-05\nEpoch 54/100\n\u001b[1m403/403\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 292ms/step - accuracy: 0.9550 - loss: 0.1119 - mean_iou: 0.8450 - val_accuracy: 0.9309 - val_loss: 0.2998 - val_mean_iou: 0.5417 - learning_rate: 1.2843e-05\nEpoch 55/100\n\u001b[1m403/403\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 292ms/step - accuracy: 0.9501 - loss: 0.1170 - mean_iou: 0.8351 - val_accuracy: 0.9282 - val_loss: 0.3032 - val_mean_iou: 0.5445 - learning_rate: 9.5492e-06\nEpoch 56/100\n\u001b[1m403/403\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 295ms/step - accuracy: 0.9492 - loss: 0.1163 - mean_iou: 0.8365 - val_accuracy: 0.9351 - val_loss: 0.3058 - val_mean_iou: 0.8617 - learning_rate: 6.6987e-06\nEpoch 57/100\n\u001b[1m403/403\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 231ms/step - accuracy: 0.9537 - loss: 0.1108 - mean_iou: 0.8568","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"# ─────────────────────────────────────────\n# 6) Plot backbone comparison\n# ─────────────────────────────────────────\nplt.figure(figsize=(6,4))\nnames  = list(results.keys())\nscores = [np.mean(results[n]) for n in names]\nplt.bar(names, scores, color=['#a6cee3','#1f78b4','#b2df8a'])\nplt.ylabel(\"Mean Val mIoU\")\nplt.ylim(0,1)\nfor i, v in enumerate(scores):\n    plt.text(i, v+0.01, f\"{v:.3f}\", ha='center')\nplt.title(\"Backbone comparison: EffB1 vs EffB2 vs EffB3\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-24T19:00:37.355Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Install dependencies\n!pip install roboflow scikit-learn pandas\n\nimport os, cv2, math, numpy as np, pandas as pd\nfrom glob import glob\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.layers import (\n    Conv2D, BatchNormalization, Activation,\n    UpSampling2D, Concatenate, Dropout,\n    Input, Lambda, Multiply\n)\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import (\n    ModelCheckpoint, ReduceLROnPlateau, LearningRateScheduler\n)\nfrom tensorflow.keras.metrics import MeanIoU\nfrom sklearn.model_selection import KFold\nfrom roboflow import Roboflow\n\n# ─────────────────────────────────────────\n# 0) Download Roboflow YOLO dataset\n# ─────────────────────────────────────────\nrf = Roboflow(api_key=\"qz3gXKPbOfhvVaI8oDt4\")\nproject = rf.workspace(\"rkm-nnbdx\").project(\"microfocus\")\nversion = project.version(8)\ndataset = version.download(\"yolov7\")  # images + labels/*.txt\n\nBASE       = dataset.location            # e.g. \"/kaggle/working/Microfocus-8\"\nIMG_SIZE   = (384, 384)\nBATCH_SIZE = 8\nAUTOTUNE   = tf.data.AUTOTUNE\nEPOCHS     = 100\nSEED       = 42\nLR         = 1e-4\nFOLDS      = 2\nSMOOTH_W   = 7\n\ntf.random.set_seed(SEED)\nnp.random.seed(SEED)\n\n# ─────────────────────────────────────────\n# 1) Generate PNG masks from YOLO polygons\n# ─────────────────────────────────────────\ndef yolo_poly_to_mask(img_path, label_path, out_path, size=IMG_SIZE):\n    w,h = size\n    mask = np.zeros((h, w), dtype=np.uint8)\n    if os.path.exists(label_path):\n        for line in open(label_path):\n            pts = np.array(list(map(float, line.split()[1:])), dtype=np.float32).reshape(-1,2)\n            pts[:,0] *= w; pts[:,1] *= h\n            cv2.fillPoly(mask, [pts.astype(np.int32)], 255)\n    cv2.imwrite(out_path, mask)\n\nfor split in (\"train\",\"valid\",\"test\"):\n    img_dir = os.path.join(BASE, split, \"images\")\n    lbl_dir = os.path.join(BASE, split, \"labels\")\n    msk_dir = os.path.join(BASE, split, \"masks\")\n    os.makedirs(msk_dir, exist_ok=True)\n    for img_p in tqdm(glob(f\"{img_dir}/*.jpg\"), desc=f\"Make masks for {split}\"):\n        fn    = os.path.splitext(os.path.basename(img_p))[0]\n        lbl_p = os.path.join(lbl_dir, fn + \".txt\")\n        out_p = os.path.join(msk_dir, fn + \".png\")\n        yolo_poly_to_mask(img_p, lbl_p, out_p)\n\n# ─────────────────────────────────────────\n# 2) Data pipeline utilities\n# ─────────────────────────────────────────\ndef load_pair(img_path, mask_path):\n    img = tf.io.read_file(img_path)\n    img = tf.image.decode_jpeg(img, 3)\n    img = tf.image.resize(img, IMG_SIZE) / 255.0\n\n    m = tf.io.read_file(mask_path)\n    m = tf.image.decode_png(m, 1)\n    m = tf.image.resize(m, IMG_SIZE, method=\"nearest\")\n    m = tf.cast(m > 127, tf.float32)\n    return img, m\n\ndef augment(img, m):\n    cat = tf.concat([img, m], axis=-1)\n    if tf.random.uniform([]) > 0.5: cat = tf.image.flip_left_right(cat)\n    if tf.random.uniform([]) > 0.5: cat = tf.image.flip_up_down(cat)\n    k = tf.random.uniform([], 0, 4, tf.int32)\n    cat = tf.image.rot90(cat, k)\n    im, ma = cat[...,:3], cat[...,3:]\n    im = tf.image.random_brightness(im, 0.1)\n    im = tf.image.random_contrast(im, 0.9, 1.1)\n    ma = tf.cast(ma>0.5, tf.float32)\n    return im, ma\n\n# **Single** make_ds signature:\ndef make_ds(img_list, mask_list, train=False):\n    ds = tf.data.Dataset.from_tensor_slices((img_list, mask_list))\n    ds = ds.map(load_pair, num_parallel_calls=AUTOTUNE)\n    if train:\n        ds = ds.map(augment, num_parallel_calls=AUTOTUNE)\n        ds = ds.shuffle(200, seed=SEED).batch(BATCH_SIZE).repeat()\n    else:\n        ds = ds.batch(BATCH_SIZE)\n    return ds.prefetch(AUTOTUNE)\n\n# Build train/val/test sets (not used in backbone CV, but here for sanity)\ntrain_ds = make_ds(glob(f\"{BASE}/train/images/*.jpg\"),\n                   [p.replace(\"/images/\",\"/masks/\").replace(\".jpg\",\".png\")\n                    for p in glob(f\"{BASE}/train/images/*.jpg\")],\n                   train=True)\nval_ds   = make_ds(glob(f\"{BASE}/valid/images/*.jpg\"),\n                   [p.replace(\"/images/\",\"/masks/\").replace(\".jpg\",\".png\")\n                    for p in glob(f\"{BASE}/valid/images/*.jpg\")],\n                   train=False)\ntest_ds  = make_ds(glob(f\"{BASE}/test/images/*.jpg\"),\n                   [p.replace(\"/images/\",\"/masks/\").replace(\".jpg\",\".png\")\n                    for p in glob(f\"{BASE}/test/images/*.jpg\")],\n                   train=False)\n\n# ─────────────────────────────────────────\n# 3) Loss & metrics\n# ─────────────────────────────────────────\ndef dice_loss(y_true,y_pred, smooth=1e-6):\n    yt, yp = K.flatten(y_true), K.flatten(y_pred)\n    inter = K.sum(yt*yp)\n    return 1 - (2*inter+smooth)/(K.sum(yt)+K.sum(yp)+smooth)\n\ndef tversky(y_true,y_pred, alpha=0.3,beta=0.7,smooth=1e-6):\n    yt, yp = K.flatten(y_true), K.flatten(y_pred)\n    tp = K.sum(yt*yp); fp = K.sum((1-yt)*yp); fn = K.sum(yt*(1-yp))\n    return (tp+smooth)/(tp+alpha*fp+beta*fn+smooth)\n\ndef focal_tversky(y_true,y_pred):\n    t = tversky(y_true,y_pred)\n    return K.pow((1-t),0.75)\n\ndef combined_loss(y_true,y_pred):\n    return 0.4*dice_loss(y_true,y_pred) + 0.6*focal_tversky(y_true,y_pred)\n\nmetrics = ['accuracy', MeanIoU(num_classes=2,name='mean_iou')]\n\n# ─────────────────────────────────────────\n# 4) Attention‑U‑Net builder\n# ─────────────────────────────────────────\ndef attention_gate(skip, g, inter):\n    θ = Conv2D(inter,1,padding='same')(skip)\n    φ = Conv2D(inter,1,padding='same')(g)\n    up= Lambda(lambda x: tf.image.resize(x[0], tf.shape(x[1])[1:3]))([φ, skip])\n    f = Activation('relu')(θ+up)\n    ψ = Conv2D(1,1,padding='same',activation='sigmoid')(f)\n    return Multiply()([skip, ψ])\n\ndef build_effattunet(backbone):\n    inp = Input((*IMG_SIZE,3))\n    enc = backbone(include_top=False, weights='imagenet', input_tensor=inp)\n    skips = [enc.get_layer(n).output for n in (\n      \"block2a_expand_activation\",\n      \"block3a_expand_activation\",\n      \"block4a_expand_activation\",\n      \"block6a_expand_activation\"\n    )]\n    x = enc.output\n    filters=[256,128,64,32]\n    for i, skip in enumerate(reversed(skips)):\n        x = UpSampling2D()(x)\n        ag = attention_gate(skip, x, inter=filters[i]//2)\n        x  = Concatenate()([x, ag])\n        x  = Conv2D(filters[i],3,padding='same',activation='relu')(x)\n        x  = BatchNormalization()(x)\n        x  = Conv2D(filters[i],3,padding='same',activation='relu')(x)\n        x  = BatchNormalization()(x)\n        x  = Dropout(0.3)(x)\n    # final upsampling to full resolution\n    x = UpSampling2D()(x)\n    x = Conv2D(32,3,padding='same',activation='relu')(x)\n    x = BatchNormalization()(x)\n    x = Conv2D(32,3,padding='same',activation='relu')(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.3)(x)\n    out = Conv2D(1,1,activation='sigmoid')(x)\n    return Model(inp,out)\n\n# ─────────────────────────────────────────\n# 5) Cross‑validation per backbone\n# ─────────────────────────────────────────\nbackbones = {\n  'EffB1': tf.keras.applications.EfficientNetB1,\n  'EffB2': tf.keras.applications.EfficientNetB2,\n  'EffB3': tf.keras.applications.EfficientNetB3\n}\n\nall_imgs  = sorted(glob(f\"{BASE}/train/images/*.jpg\"))\nall_masks = [p.replace(\"/images/\",\"/masks/\").replace(\".jpg\",\".png\")\n             for p in all_imgs]\nkf = KFold(n_splits=FOLDS, shuffle=True, random_state=SEED)\n\nresults = {name: [] for name in backbones}\nfor name, eff in backbones.items():\n    print(f\"\\n=== {name} ===\")\n    for fold, (trIdx, valIdx) in enumerate(kf.split(all_imgs)):\n        print(f\" Fold {fold+1}\")\n        trI = [all_imgs[i] for i in trIdx];  trM = [all_masks[i] for i in trIdx]\n        vI  = [all_imgs[i] for i in valIdx]; vM  = [all_masks[i] for i in valIdx]\n\n        dsTr  = make_ds(trI, trM, train=True)\n        dsVal = make_ds(vI, vM, train=False)\n\n        model = build_effattunet(eff)\n        model.compile(optimizer=Adam(LR),\n                      loss=combined_loss,\n                      metrics=metrics)\n\n        def cos_lr(e):\n            t = e % 30\n            return LR * 0.5 * (1 + math.cos(math.pi * t/30))\n        cbs = [\n          LearningRateScheduler(cos_lr, verbose=0),\n          ModelCheckpoint( f\"{name}_f{fold}.h5\",\n                           monitor=\"val_mean_iou\", mode=\"max\",\n                           save_best_only=True),\n          ReduceLROnPlateau( monitor=\"val_mean_iou\",\n                            factor=0.5, patience=10,\n                            mode=\"max\", verbose=0)\n        ]\n\n        steps = len(trI)//BATCH_SIZE\n        vstps = len(vI)//BATCH_SIZE\n        h = model.fit(dsTr,\n                      epochs=EPOCHS,\n                      steps_per_epoch=steps,\n                      validation_data=dsVal,\n                      validation_steps=vstps,\n                      callbacks=cbs,\n                      verbose=1)\n        model.load_weights(f\"{name}_f{fold}.h5\")\n        results[name].append(max(h.history['mean_iou']))\n\n    print(\" →\", name, \"mean IoU:\", np.mean(results[name]))\n\n# ─────────────────────────────────────────\n# 6) Plot backbone comparison\n# ─────────────────────────────────────────\nplt.figure(figsize=(6,4))\nnames  = list(results.keys())\nscores = [np.mean(results[n]) for n in names]\nplt.bar(names, scores, color=['#a6cee3','#1f78b4','#b2df8a'])\nplt.ylabel(\"mIoU\")\nplt.ylim(0,1)\nfor i, v in enumerate(scores):\n    plt.text(i, v+0.01, f\"{v:.3f}\", ha='center')\nplt.title(\"Backbone comparison: EffB1 vs EffB2 vs EffB3\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-22T07:11:03.487076Z","iopub.execute_input":"2025-06-22T07:11:03.487439Z","execution_failed":"2025-06-22T07:12:07.511Z"}},"outputs":[{"name":"stdout","text":"Collecting roboflow\n  Downloading roboflow-1.1.66-py3-none-any.whl.metadata (9.7 kB)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.2.2)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.3)\nRequirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from roboflow) (2025.4.26)\nCollecting idna==3.7 (from roboflow)\n  Downloading idna-3.7-py3-none-any.whl.metadata (9.9 kB)\nRequirement already satisfied: cycler in /usr/local/lib/python3.11/dist-packages (from roboflow) (0.12.1)\nRequirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.4.8)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from roboflow) (3.7.2)\nRequirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.26.4)\nCollecting opencv-python-headless==4.10.0.84 (from roboflow)\n  Downloading opencv_python_headless-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\nRequirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from roboflow) (11.1.0)\nCollecting pillow-heif>=0.18.0 (from roboflow)\n  Downloading pillow_heif-0.22.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\nRequirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.9.0.post0)\nCollecting python-dotenv (from roboflow)\n  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.32.3)\nRequirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.17.0)\nRequirement already satisfied: urllib3>=1.26.6 in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.4.0)\nRequirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from roboflow) (4.67.1)\nRequirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from roboflow) (6.0.2)\nRequirement already satisfied: requests-toolbelt in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.0.0)\nCollecting filetype (from roboflow)\n  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.2)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.18.5->roboflow) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.18.5->roboflow) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.18.5->roboflow) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.18.5->roboflow) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.18.5->roboflow) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.18.5->roboflow) (2.4.1)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (1.3.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (4.57.0)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (25.0)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->roboflow) (3.4.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.18.5->roboflow) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.18.5->roboflow) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.18.5->roboflow) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.18.5->roboflow) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.18.5->roboflow) (2024.2.0)\nDownloading roboflow-1.1.66-py3-none-any.whl (86 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.7/86.7 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading idna-3.7-py3-none-any.whl (66 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.8/66.8 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading opencv_python_headless-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.9/49.9 MB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading pillow_heif-0.22.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m102.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\nDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\nInstalling collected packages: filetype, python-dotenv, pillow-heif, idna, opencv-python-headless, roboflow\n  Attempting uninstall: idna\n    Found existing installation: idna 3.10\n    Uninstalling idna-3.10:\n      Successfully uninstalled idna-3.10\n  Attempting uninstall: opencv-python-headless\n    Found existing installation: opencv-python-headless 4.11.0.86\n    Uninstalling opencv-python-headless-4.11.0.86:\n      Successfully uninstalled opencv-python-headless-4.11.0.86\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.3.2 which is incompatible.\ngoogle-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.40.1 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\ngoogle-spark-connect 0.5.2 requires google-api-core>=2.19.1, but you have google-api-core 1.34.1 which is incompatible.\ngoogle-cloud-bigtable 2.30.0 requires google-api-core[grpc]<3.0.0,>=2.16.0, but you have google-api-core 1.34.1 which is incompatible.\ngoogle-cloud-storage 2.19.0 requires google-api-core<3.0.0dev,>=2.15.0, but you have google-api-core 1.34.1 which is incompatible.\npandas-gbq 0.28.0 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed filetype-1.2.0 idna-3.7 opencv-python-headless-4.10.0.84 pillow-heif-0.22.0 python-dotenv-1.1.0 roboflow-1.1.66\n","output_type":"stream"},{"name":"stderr","text":"2025-06-22 07:11:13.999996: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1750576274.202637      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1750576274.260018      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"loading Roboflow workspace...\nloading Roboflow project...\n","output_type":"stream"},{"name":"stderr","text":"Make masks for train: 100%|██████████| 1314/1314 [00:00<00:00, 1371.05it/s]\nMake masks for valid: 100%|██████████| 128/128 [00:00<00:00, 1304.58it/s]\nMake masks for test: 100%|██████████| 72/72 [00:00<00:00, 1278.81it/s]\nI0000 00:00:1750576290.135178      35 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n","output_type":"stream"},{"name":"stdout","text":"\n=== EffB1 ===\n Fold 1\nDownloading data from https://storage.googleapis.com/keras-applications/efficientnetb1_notop.h5\n\u001b[1m27018416/27018416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\nEpoch 1/100\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import roc_curve, auc\n\n# 1) Plot Training vs Validation Curves\n# --------------------------------------\nepochs = range(1, len(h.history['loss']) + 1)\n\nplt.figure(figsize=(12,4))\nplt.subplot(1,3,1)\nplt.plot(epochs, h.history['loss'],    label='Train Loss')\nplt.plot(epochs, h.history['val_loss'],label='Val Loss')\nplt.title(\"Loss\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.legend()\n\nplt.subplot(1,3,2)\nplt.plot(epochs, h.history['accuracy'],     label='Train Acc')\nplt.plot(epochs, h.history['val_accuracy'], label='Val Acc')\nplt.title(\"Accuracy\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Acc\")\nplt.legend()\n\nplt.subplot(1,3,3)\nplt.plot(epochs, h.history['mean_iou'],      label='Train mIoU')\nplt.plot(epochs, h.history['val_mean_iou'],  label='Val mIoU')\nplt.title(\"Mean IoU\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"mIoU\")\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\n\n# # 2) Compute & Plot ROC on the Test Set\n# # -------------------------------------\n# y_trues = []\n# y_probs = []\n\n# for imgs, masks in test_ds:\n#     preds = best.predict(imgs, verbose=0)  # shape (B, H, W, 1)\n#     y_trues.append(masks.numpy().ravel())\n#     y_probs.append(preds.ravel())\n\n# y_true = np.concatenate(y_trues)\n# y_score = np.concatenate(y_probs)\n\n# # If your test set is large you might subsample:\n# # idx = np.random.choice(len(y_true), size=200_000, replace=False)\n# # y_true, y_score = y_true[idx], y_score[idx]\n\n# fpr, tpr, _ = roc_curve(y_true, y_score)\n# roc_auc = auc(fpr, tpr)\n\n# plt.figure(figsize=(6,6))\n# plt.plot(fpr, tpr, lw=2, label=f\"ROC AUC = {roc_auc:.3f}\")\n# plt.plot([0,1],[0,1], linestyle='--', color='gray')\n# plt.xlabel(\"False Positive Rate\")\n# plt.ylabel(\"True Positive Rate\")\n# plt.title(\"ROC Curve (pixel‑wise)\")\n# plt.legend(loc=\"lower right\")\n# plt.show()\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-22T07:12:07.512Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom tensorflow.keras.models import load_model\nfrom tensorflow.keras.metrics import MeanIoU\n\n# 1. Define your custom losses and metrics used during training\ndef dice_loss(y_true,y_pred, smooth=1e-6):\n    yt, yp = tf.keras.backend.flatten(y_true), tf.keras.backend.flatten(y_pred)\n    inter = tf.keras.backend.sum(yt*yp)\n    return 1 - (2*inter+smooth)/(tf.keras.backend.sum(yt)+tf.keras.backend.sum(yp)+smooth)\n\ndef tversky(y_true,y_pred, alpha=0.3,beta=0.7,smooth=1e-6):\n    yt, yp = tf.keras.backend.flatten(y_true), tf.keras.backend.flatten(y_pred)\n    tp = tf.keras.backend.sum(yt*yp)\n    fp = tf.keras.backend.sum((1-yt)*yp)\n    fn = tf.keras.backend.sum(yt*(1-yp))\n    return (tp+smooth)/(tp+alpha*fp+beta*fn+smooth)\n\ndef focal_tversky(y_true,y_pred):\n    t = tversky(y_true,y_pred)\n    return tf.keras.backend.pow((1-t),0.75)\n\ndef combined_loss(y_true,y_pred):\n    return 0.4*dice_loss(y_true,y_pred) + 0.6*focal_tversky(y_true,y_pred)\n\n# 2. Load best trained model (EffB3, fold 1 for example)\nbest_model_path = \"EffB3_f1.h5\"\nmodel = load_model(best_model_path,\n                   custom_objects={\n                       \"combined_loss\": combined_loss,\n                       \"MeanIoU\": MeanIoU,\n                       \"dice_loss\": dice_loss,\n                       \"tversky\": tversky,\n                       \"focal_tversky\": focal_tversky\n                   })\n\n# 3. Load training history if you have it saved, or use the variable `h` if in memory\n# Example: assume h.history is available\n# You can also save history during training using: `pd.DataFrame(h.history).to_csv(...)`\n\n# If you already have `h = model.fit(...)` earlier:\nhistory = h.history  # replace this if loading from CSV: pd.read_csv('history.csv')\n\n# 4. Smooth helper\ndef smooth(values, weight=0.8):\n    smoothed = []\n    last = values[0]\n    for v in values:\n        smoothed_val = last * weight + (1 - weight) * v\n        smoothed.append(smoothed_val)\n        last = smoothed_val\n    return smoothed\n\n# 5. Plot curves\nepochs = range(1, len(history['loss']) + 1)\n\nplt.figure(figsize=(18,5))\n\n# Loss\nplt.subplot(1, 3, 1)\nplt.plot(epochs, smooth(history['loss']), label='Train Loss (smoothed)')\nplt.plot(epochs, smooth(history['val_loss']), label='Val Loss (smoothed)')\nplt.title(\"Loss per Epoch\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.legend()\n\n# Accuracy\nplt.subplot(1, 3, 2)\nplt.plot(epochs, smooth(history['accuracy']), label='Train Acc (smoothed)')\nplt.plot(epochs, smooth(history['val_accuracy']), label='Val Acc (smoothed)')\nplt.title(\"Accuracy per Epoch\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\n\n# mIoU\nplt.subplot(1, 3, 3)\nplt.plot(epochs, smooth(history['mean_iou']), label='Train mIoU (smoothed)')\nplt.plot(epochs, smooth(history['val_mean_iou']), label='Val mIoU (smoothed)')\nplt.title(\"Mean IoU per Epoch\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"mIoU\")\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-22T07:12:07.512Z"}},"outputs":[],"execution_count":null}]}